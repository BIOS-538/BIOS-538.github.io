---
title: "lecture13"
author: "Prashant K"
date: 27/Feb/24
format: 
  revealjs:
    smaller: true
metadata-files: 
  - '_slides.yml'
editor: visual
---

# Recap

Last class covered

-   How normal distributions are generated: Sum of random variables / `mean` of a random sample

    -   Central limit theorem

-   Used R to simulate random events and explore distributions

# Today's class

-   *Finish segment*: How do I [test](https://www.statology.org/test-for-normality-in-r/) if my data is normal? `q-q plots` and `Shapiro-Wilk` test

-   What questions do **t-tests** answer?

-   t-tests dissected: Layout of a general **hypothesis test**

-   Understand **sampling distributions**, standard error of mean (SEM) and confidence intervals (CI)

# Checking for normality

How to check if my data is normally distributed?

-   We need metrics that give us the shape of the distribution without having to plot the distribution and staring at it

```{r setup}
library(tidyverse)  # for tibbles, and pipes
theme_set(theme_classic()) # set theme with plain background for all plots
gapminder <- gapminder::gapminder # get gapminder data
```

## Quantifying the shape of the normal distribution {.smaller}

::: columns
::: {.column width="50%"}
**Mean +/- SD**

![](img/normal_dist_probabilities_sd.png){width="400"}

> Source: [Wikipedia](https://en.wikipedia.org/wiki/File:Standard_deviation_diagram_micro.svg)
:::

::: {.column width="50%"}
**Median, quartiles**

![](img/normal_dist_quartiles.jpg){width="355"}

::: aside
> Source: Quartiles of Normal Distribution, [JoeCMath/youtube](https://www.youtube.com/watch?v=0uef_Bw4xfM)
:::

**quartiles** divide the probability / distribution area into 4 **quarters,** a more general way is called **quantile** (*note the spelling!*)
:::
:::

## Quantiles tell us about the distribution shape  {.smaller}

::: columns
::: {.column width="50%"}
Quartiles (4)

```{r dist_w_quantiles}
#| fig-height: 3

# Data for a simple non-normal distribution for quantile demonstration
life_exp <- gapminder$lifeExp
quartiles <- quantile(life_exp)
deciles <- quantile(life_exp, probs = seq(0, 1, 0.1))

dist_plt <- 
  ggplot(mapping = aes(gapminder$lifeExp)) + 
  geom_histogram(aes(y = after_stat(density)), alpha = 0.5) +  # scaled histogram (probability dist)
  geom_density() # smooth dist

dist_plt +  
  # show quartiles
  stat_summary(aes(y = 0.04, xintercept = after_stat(x)), 
               fun = quantile, 
               # fun.args = list(probs = c(0.025, 0.975)), 
               geom = "vline", 
               alpha = 0.5, linetype = 2,
               orientation = "y")

# to plot quantiles : https://stackoverflow.com/a/30569539/9049673
```
:::

::: {.column width="50%"}
Deciles (10)

```{r dist-w-declies}
#| fig-height: 3 

dist_plt +  
  # show quartiles
  stat_summary(aes(y = 0.04, xintercept = after_stat(x)), 
               fun = quantile, 
               fun.args = list(probs = seq(0, 1, 0.1)),
               geom = "vline", 
               alpha = 0.5, linetype = 2,
               orientation = "y")

```
:::
:::

> **quantiles** are cut points dividing the [range](https://en.wikipedia.org/wiki/Range_(statistics) "Range (statistics)") of a [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution "Probability distribution") into continuous intervals with equal probabilities. Source: [Wikipedia](https://en.wikipedia.org/wiki/Quantile)

probability distribution = `geom_density` plot. Hence quantiles divide the range to make equal area under the `geom_density` curve

## `Q-Q` plot enables comparison of distribution shapes

If they fall along the line, then it is normal.

::: columns
::: {.column width="50%"}
Normal distribution

```{r qq-plot-normal}
#| fig-height: 4
rnorm(1000) %>% 
car::qqPlot()
```
:::

::: {.column width="50%"}
Non-normal distribution

```{r qq-plot}
#| fig-height: 4
car::qqPlot(life_exp)
```
:::
:::

::: aside
further reading : [sthda](http://www.sthda.com/english/wiki/qq-plots-quantile-quantile-plots-r-base-graphs)
:::

## Formal `Shapiro-Wilk` test for normality

What is this testing for?

-   NULL hypothesis is that data is normally distributed
-   High p-value =\> Data is normal (*hypothesis is accepted*)

::: columns
::: {.column width="50%"}
### Normal distribution

```{r shapiro-wilk-normal}
set.seed = 1
rnorm(1000) %>% 
  shapiro.test()
```
:::

::: {.column width="50%"}
### Non-normal distribution

```{r shapiro-wilk}

shapiro.test(life_exp)
```
:::
:::

> further reading: [statology.org](https://www.statology.org/test-for-normality-in-r/)

# What are t-tests for?

To make statistical conclusions of this kind based on data

-   "Smoking **causes** cancer"

But causality is really really hard to establish, that is why we talk about [correlation/association](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation)

-   "Smoking **is associated with** cancer"

-   "Smokers are **statistically significantly** at a higher risk of having cancer"

> "Evidence **linking** smoking and cancer appeared in the 1920s." Source: [The cigarette controversy, 2007](https://aacrjournals.org/cebp/article/16/6/1070/260310/The-Cigarette-Controversy#10539424)/section: *Smoking Causes Cancer: When Did They Know?*

## What data can test the smoking hypothesis?

::: columns
::: {.column width="50%"}
2d data

```{r linreg}
set.seed = 1
smk_2d <- 
  tibble(smokindex = rep(1:10, each = 10),
         cancer_risk = (3 * smokindex + rnorm(100, mean = 5, sd = 3)) %>% 
           scales::rescale())

ggplot(smk_2d,
       aes(smokindex, cancer_risk)) + 
  geom_point() + geom_smooth(method = 'lm') + 
  labs(x = 'smoking index', y = 'cancer risk')

```

-   Correlation

-   Regression (linear/non-linear)
:::

::: {.column width="50%"}
1D data

```{r num_cat_smoking}
smk <- tibble(id = 1:12, 
              cancer_index = c(rnorm(6, mean = 2), rnorm(6, mean = 3.5)),
              smoking_status = rep(c(F, T), each = 6)) %>% 
  mutate(mean_cancer = mean(cancer_index), .by = smoking_status)


ggplot(smk, aes(smoking_status, cancer_index)) + 
  geom_point(alpha = 0.5, 
             position = position_jitter(width = 0.2, height = 0)) + 
  
  # show mean
  geom_point(aes(y = mean_cancer), 
             shape = '_', size = 20) + 
  geom_text(aes(y = mean_cancer + .2, label = round(mean_cancer, 2)))

```

-   t-test
:::
:::

## Comparing two numbers vs two samples

Looking at the mean only is not enough:

`r smk$mean_cancer[7] %>% round(2)` is clearly **\>** `r smk$mean_cancer[1] %>% round(2)`, does that alone satisfy the hypothesis?

-   We need to account for both the mean and variability, ie) the spread around the mean

This is what the t-statistic encompasses. The t-test tests for a hypothesis based on this statistic

## t-tests finds statistical support against a NULL hypothesis

Null hypothesis in english - "smoking is **NOT** associated with cancer"

In statistical terms - "the samples of smokers and non-smokers came from the **same population**"

-   (*assumption)* For a t-test, this source population is normally distributed

-   *(Other assumption for Student's t-test)* Equal variance between smoker dataset and non-smoker dataset (this can be violated for the `welch t-test`)

## t-test is calculating difference in means / spread around mean

![](img/t-test_formula_sketched.png)

Watch full video here: "*Student's t-test*" : [Bozeman science/youtube](https://www.youtube.com/watch?v=pTmLQvMM-1M)

# Sampling distributions {.incremental .smaller}

::: incremental
1.  Sample multiple times from a population (*too expensive for a real experiment, so we imagine this*)
2.  Calculate one statistic from the sample (such as `mean`) and record it (*for each sample)*
3.  Plot the statistic across all samples. The statistic is a random variable too dependent on the random sample, hence results in a different value for each sample.
4.  This results in a distribution (called **sampling distribution** of `mean`*/statistic*)
    -   `mean` is nice because it follows a normal distribution (due to the *central limit theorem*!)
        -   it's mean \~ close approximation to the population mean
        -   it's standard deviation = standard error of mean (**S.E.M.**) \~ $\sigma$
5.  The properties of this distribution are helpful to understand the population which is what we are interested in
:::

## Sampling from a population of coloured balls

We will walk through this activity to understand sampling well

-   Explanation of sampling with balls in [moderndive/chap7](https://moderndive.com/7-sampling.html#what-proportion-of-this-bowls-balls-are-red)

## Let us see the sampling distribution

## Calculating the sample mean, SEM, CI

## Relating this to a 1-sample t-test

## References

Great explanation of sample size in 7.2 (FIGURE 7.12) in [moderndive/chap7](https://moderndive.com/7-sampling.html#what-proportion-of-this-bowls-balls-are-red)

# Summary

-   `q-q plots` and `Shapiro-Wilk` test enable checking data for normality

-   t-tests answer questions of association such as '*smoking causes cancer*'

-   t-test = **p-value** gives the probability that the NULL hypothesis: *that both data groups were sampled from the same distribution* is `TRUE`

-   Understood sampling distributions, standard error of mean (SEM) and confidence intervals (CI)
